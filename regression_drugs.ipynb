{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24797468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "579ed6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_file(file_path, text_columns_start_row=0):\n",
    "    \"\"\"\n",
    "    Processes a text file to handle numeric columns and filter NaN values.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: Path to the text file.\n",
    "    - text_columns_start_row: Row index from where to start processing text columns (default is 1).\n",
    "    \n",
    "    Returns:\n",
    "    - recombined_df: A DataFrame with filtered numeric and text columns.\n",
    "    \"\"\"\n",
    "    # Step 1: Load the data into a pandas DataFrame\n",
    "    df = pd.read_csv(file_path, delimiter='\\t', low_memory=False)\n",
    "    \n",
    "    # Step 2: Select the first two text columns\n",
    "    df_first_two_columns = df.iloc[text_columns_start_row:, :2]  # First two columns\n",
    "    \n",
    "    # Step 3: Select the numeric columns (all except the first two and last two)\n",
    "    df_numeric = df.iloc[text_columns_start_row:, 2:-2]  # Select numeric columns dynamically\n",
    "    \n",
    "    # Step 4: Apply to_numeric to convert non-numeric values to NaN. Exclude the description row and then add it back.\n",
    "    first_row = df_numeric.iloc[0]\n",
    "    rest_of_df = df_numeric.iloc[1:]\n",
    "    df_numeric = rest_of_df.apply(pd.to_numeric, errors='coerce')\n",
    "    df_numeric = pd.concat([pd.DataFrame([first_row]), df_numeric], ignore_index=True)\n",
    "\n",
    "    # Step 5: Retain the last two text columns\n",
    "    df_last_two_columns = df.iloc[text_columns_start_row:, -2:]  # Last two columns\n",
    "    \n",
    "    # Step 6: Filter rows with no NaN values in the numeric columns\n",
    "    non_nan_filtered = df_numeric.dropna()\n",
    "    \n",
    "    # Step 7: Filter out columns that have all NaN values in the numeric subset\n",
    "\n",
    "    # Function to check if all elements except the first one are NaN\n",
    "    def should_drop(column):\n",
    "        return column[1:].isna().all()\n",
    "\n",
    "    # Identify columns to drop\n",
    "    columns_to_drop = [col for col in df_numeric.columns if should_drop(df_numeric[col])]\n",
    "\n",
    "    # Drop those columns\n",
    "    non_nan_columns = df_numeric.drop(columns=columns_to_drop)\n",
    "    \n",
    "    # Step 8: Recombine the numeric subset, first two text columns, and last two text columns\n",
    "    recombined_df = pd.concat([df_first_two_columns, non_nan_columns, df_last_two_columns], axis=1)\n",
    "\n",
    "    # Step 9: Check that collection_title or promoted_subjectkey are both there, otherwise print a warning\n",
    "    description_row = recombined_df.iloc[0]\n",
    "    desc_contains_collection_title = description_row.apply(lambda x: 'collection_title' in str(x)).any()\n",
    "    desc_contains_promoted_subjectkey = description_row.apply(lambda x: 'promoted_subjectkey' in str(x)).any()\n",
    "    cols_contain_collection_title = 'collection_title' in recombined_df.columns\n",
    "    cols_contain_promoted_subjectkey = 'promoted_subjectkey' in recombined_df.columns\n",
    "\n",
    "    if not desc_contains_collection_title:\n",
    "        print(\"Collection_title is missing from the column descriptions.\")\n",
    "    if not desc_contains_promoted_subjectkey:\n",
    "        print(\"Promoted_subjectkey is missing from the column descriptions.\")\n",
    "    if not cols_contain_collection_title:\n",
    "        print(\"Collection_title is missing from the column names.\")\n",
    "    if not cols_contain_promoted_subjectkey:\n",
    "        print(\"Promoted_subjectkey is missing from the column names.\")\n",
    "    if desc_contains_collection_title and desc_contains_promoted_subjectkey and cols_contain_collection_title and cols_contain_promoted_subjectkey:\n",
    "        return recombined_df\n",
    "\n",
    "def process_all_files(root_folder, file_list, text_columns_start_row=0):\n",
    "    \"\"\"\n",
    "    Processes all files in the list and returns the processed DataFrames.\n",
    "    \n",
    "    Parameters:\n",
    "    - root_folder: Root folder containing all text files.\n",
    "    - file_list: List of file names to process.\n",
    "    - text_columns_start_row: Row index from where to start processing text columns (default is 1).\n",
    "    \n",
    "    Returns:\n",
    "    - result_dict: Dictionary with file names as keys and processed DataFrames as values.\n",
    "    \"\"\"\n",
    "    result_dict = {}\n",
    "    \n",
    "    for file_name in file_list:\n",
    "        file_path = os.path.join(root_folder, file_name)\n",
    "        print(f\"Processing {file_name}...\")\n",
    "        \n",
    "        # Process the file\n",
    "        processed_df = process_file(file_path, text_columns_start_row)\n",
    "        \n",
    "        # Store the result in the dictionary\n",
    "        if isinstance(processed_df, pd.DataFrame):\n",
    "            result_dict[file_name] = processed_df\n",
    "    \n",
    "    return result_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eda60543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aesposys01.txt', 'aims01.txt', 'cata01.txt', 'cgis01.txt', 'clgry01.txt', 'dai01.txt', 'demo01.txt', 'dgsposys01.txt', 'dosecomp01.txt', 'ecg01.txt', 'endphase01.txt', 'endstudy01.txt', 'fint01.txt', 'hair01.txt', 'itaq01.txt', 'keyvars01.txt', 'lab01.txt', 'maccomp01.txt', 'macvlnce01.txt', 'med01.txt', 'meddispn01.txt', 'ndar_aggregate.txt', 'ndar_subject01.txt', 'neurobatt01.txt', 'package_info.txt', 'panss01.txt', 'qol01.txt', 'sae01.txt', 'scid_ph01.txt', 'screen01.txt', 'sf1201.txt', 'surf01.txt', 'surfq01.txt', 'timeto01.txt', 'viol01.txt', 'vitals01.txt']\n"
     ]
    }
   ],
   "source": [
    "#Get a list of all text files from the root folder\n",
    "\n",
    "def get_text_files(directory):\n",
    "    text_files = [f for f in os.listdir(directory) if f.endswith('.txt') and os.path.isfile(os.path.join(directory, f))]\n",
    "    return text_files\n",
    "\n",
    "root_folder = r\"C:\\Users\\Senna\\Desktop\\Iigaya_lab\\catie\\catie_text_data\\catie\"\n",
    "file_list = get_text_files(root_folder)\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd7cb766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing aesposys01.txt...\n",
      "Processing aims01.txt...\n",
      "Processing cata01.txt...\n",
      "Processing cgis01.txt...\n",
      "Processing clgry01.txt...\n",
      "Processing dai01.txt...\n",
      "Processing demo01.txt...\n",
      "Processing dgsposys01.txt...\n",
      "Processing dosecomp01.txt...\n",
      "Processing ecg01.txt...\n",
      "Processing endphase01.txt...\n",
      "Processing endstudy01.txt...\n",
      "Processing fint01.txt...\n",
      "Processing hair01.txt...\n",
      "Processing itaq01.txt...\n",
      "Processing keyvars01.txt...\n",
      "Processing lab01.txt...\n",
      "Processing maccomp01.txt...\n",
      "Processing macvlnce01.txt...\n",
      "Processing med01.txt...\n",
      "Processing meddispn01.txt...\n",
      "Processing ndar_aggregate.txt...\n",
      "Collection_title is missing from the column descriptions.\n",
      "Promoted_subjectkey is missing from the column descriptions.\n",
      "Collection_title is missing from the column names.\n",
      "Promoted_subjectkey is missing from the column names.\n",
      "Processing ndar_subject01.txt...\n",
      "Processing neurobatt01.txt...\n",
      "Processing package_info.txt...\n",
      "Collection_title is missing from the column descriptions.\n",
      "Promoted_subjectkey is missing from the column descriptions.\n",
      "Collection_title is missing from the column names.\n",
      "Promoted_subjectkey is missing from the column names.\n",
      "Processing panss01.txt...\n",
      "Processing qol01.txt...\n",
      "Processing sae01.txt...\n",
      "Processing scid_ph01.txt...\n",
      "Processing screen01.txt...\n",
      "Processing sf1201.txt...\n",
      "Processing surf01.txt...\n",
      "Processing surfq01.txt...\n",
      "Processing timeto01.txt...\n",
      "Processing viol01.txt...\n",
      "Processing vitals01.txt...\n"
     ]
    }
   ],
   "source": [
    "# Process all files and store the results in a dictionary\n",
    "processed_data = process_all_files(root_folder, file_list)\n",
    "# processed_data['med01.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ce61e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge all dfs which have promoted_subjectkey and a collection_title\n",
    "#This is going to merge on all shared columns, doing outer to make sure all rows are kept. NaNs will go in where info does not exist for that row.\n",
    "\n",
    "# Initial DataFrame (use the first DataFrame in the dictionary)\n",
    "merged_df = list(processed_data.values())[0]\n",
    "\n",
    "# Merge remaining DataFrames\n",
    "for key in list(processed_data.keys())[1:]:\n",
    "    merged_df = pd.merge(merged_df, processed_data[key], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7702e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Treatment for Phase 1', nan, 5.0, 1.0, 3.0, 2.0, 4.0],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#other idea of what the column of medications for phase 1 is\n",
    "#merged_df.loc['med2g']\n",
    "merged_df['treat_1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9046f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each promoted subject key, get rows with non-NAN values of: treat_1, dcr_tae1, dcr_eff1\n",
    "# with that promoted subject key as 'key', find the values for 'cocaine', 'opiates', 'pcp', 'meth', 'thc'.\n",
    "# do logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58d4df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter for non-NA 'treat_1' and get relevant 'promoted_subjectkey'\n",
    "filtered_df = merged_df[merged_df['treat_1'].notna()]\n",
    "promoted_keys = filtered_df['promoted_subjectkey'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "979156a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Filter for the relevant 'promoted_subjectkey' and required columns\n",
    "selected_df = merged_df[merged_df['promoted_subjectkey'].isin(promoted_keys)][[\n",
    "    'promoted_subjectkey', 'dcr_tae1', 'dcr_eff1', 'cocaine', 'opiates', 'pcp', 'meth', 'thc'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f83fb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2b: Aggregate data by 'promoted_subjectkey'\n",
    "aggregated_df = selected_df.groupby('promoted_subjectkey').agg({\n",
    "    'dcr_tae1': 'first',  # take first non-NA occurrence of 'dcr_tae1'\n",
    "    'cocaine': 'max',     # if any row has '1', this will capture it\n",
    "    'opiates': 'max',\n",
    "    'pcp': 'max',\n",
    "    'meth': 'max',\n",
    "    'thc': 'max'\n",
    "}).dropna(subset=['dcr_tae1'])  # Ensure 'dcr_tae1' has values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fac3c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = aggregated_df[['cocaine', 'opiates', 'pcp', 'meth', 'thc']].apply(pd.to_numeric, errors='coerce')\n",
    "y = pd.to_numeric(aggregated_df['dcr_tae1'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b3fe1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([X, y], axis=1).dropna()\n",
    "X = data[['cocaine', 'opiates', 'pcp', 'meth', 'thc']]\n",
    "y = data['dcr_tae1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6882dde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.421846\n",
      "         Iterations: 35\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:               dcr_tae1   No. Observations:                 1007\n",
      "Model:                          Logit   Df Residuals:                     1001\n",
      "Method:                           MLE   Df Model:                            5\n",
      "Date:                Sun, 03 Nov 2024   Pseudo R-squ.:                0.001838\n",
      "Time:                        20:43:36   Log-Likelihood:                -424.80\n",
      "converged:                      False   LL-Null:                       -425.58\n",
      "Covariance Type:            nonrobust   LLR p-value:                    0.9055\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -1.7042      0.101    -16.934      0.000      -1.901      -1.507\n",
      "cocaine    -7.185e-05      0.000     -0.252      0.801      -0.001       0.000\n",
      "opiates       -0.0250      0.037     -0.670      0.503      -0.098       0.048\n",
      "pcp           -3.8839   2.36e+06  -1.65e-06      1.000   -4.62e+06    4.62e+06\n",
      "meth           0.0002      0.005      0.044      0.965      -0.010       0.010\n",
      "thc           -0.0742      0.219     -0.339      0.735      -0.503       0.355\n",
      "==============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:1819: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:1819: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    }
   ],
   "source": [
    "# Add constant term for intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit logistic regression model\n",
    "model = sm.Logit(y, X)\n",
    "result = model.fit()\n",
    "\n",
    "# Print the summary\n",
    "print(result.summary())\n",
    "\n",
    "\n",
    "# none of the drug variables was a significant predictor for dcr_tae1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97d2d26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.421846\n",
      "         Iterations: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    }
   ],
   "source": [
    "result = model.fit(maxiter=100) #failed to converge...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
